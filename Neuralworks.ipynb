{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar cualquier trabajo, pienso en las variables, columnas o data entregada por parte del cliente. Por lo cual se torna indispensable para mi, hacer un listado de variables en conjunto a una descripcion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "region: Indica la ciudad o región donde se originó el viaje.\n",
    "\n",
    "origin_coord: Coordenadas de origen del viaje. Formato POINT.\n",
    "\n",
    "destination_coord: Coordenadas de destino del viaje. Formato POINT.\n",
    "\n",
    "datetime: Fecha y hora cuando el viaje se realizó.\n",
    "\n",
    "datasource: Fuente del dato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Procesos automatizados para ingerir y almacenar los datos bajo demanda\n",
    "    a. Los viajes que son similares en términos de origen, destino y hora del día deben agruparse. Describa\n",
    "    el enfoque que utilizó para agregar viajes similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingesta y almacenamiento de datos: Es fundamental pensar en un proceso automatizado para leer los datos necesarios y almacenarlos en una base de datos SQL.\n",
    "Por ende, sera necesario utilizar librerias que nos brinden apoyo en estas tareas, tales como Pandas para leer la data y sqlalchemy para interactuar con la BD.\n",
    "IMPORTANTE verificar si los datos son nuevos o ya han sido almacenados previamente, para evitar duplicidad de datos e interferir con la estadistica o procesos automaticos.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Para hacer la ingesta de datos utilizaremos pandas y sqlalchemy.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Crea una conexión a la base de datos\n",
    "engine = create_engine('MiBaseDeDatos')\n",
    "\n",
    "# Lee los datos del CSV\n",
    "df = pd.read_csv('trips.csv')\n",
    "\n",
    "# Almacena los datos en la base de datos\n",
    "df.to_sql('viajes', engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Para agrupar es posible hacerlo de diversas maneras. Una de ellas es utilizar la libreria geohash2 disponible para este trabajo\n",
    "'''\n",
    "\n",
    "import geohash2\n",
    "\n",
    "def extract_coords(point_str):\n",
    "    lon, lat = map(float, point_str.strip('POINT (').strip(')').split()) #Extraemos las coordenadas de la cadena POINT.\n",
    "\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "df['origin_coord'] = df['origin_coord'].apply(lambda x: geohash2.encode(*extract_coords(x)))# Se convierten las coordenadas a geohash\n",
    "df['destination_coord'] = df['destination_coord'].apply(lambda x: geohash2.encode(*extract_coords(x)))\n",
    "\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime']).dt.round('H')#Se redondea la hora mas cercana.\n",
    "\n",
    "\n",
    "df_grouped = df.groupby(['region', 'origin_coord', 'destination_coord', 'datetime']).size().reset_index(name='count')# Agrupa los viajes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segunda Opcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tambien es posible usar la siguiente metodologia en conjunto con DBSCAN\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Crea una conexión a la base de datos\n",
    "engine = create_engine('MiBaseDeDatos')\n",
    "\n",
    "# Lee los datos del CSV\n",
    "df = pd.read_csv('trips.csv')\n",
    "\n",
    "# Almacena los datos en la base de datos\n",
    "df.to_sql('viajes', engine, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['origin_longitude', 'origin_latitude']] = df['origin_coord'].str.replace('POINT \\(', '').str.replace('\\)', '').str.split(' ', expand=True)\n",
    "df[['destination_longitude', 'destination_latitude']] = df['destination_coord'].str.replace('POINT \\(', '').str.replace('\\)', '').str.split(' ', expand=True)#Se Extraen las coordenadas de las columnas de texto\n",
    "df[['origin_longitude', 'origin_latitude', 'destination_longitude', 'destination_latitude']] = df[['origin_longitude', 'origin_latitude', 'destination_longitude', 'destination_latitude']].apply(pd.to_numeric)# Convertir a formato numerico\n",
    "\n",
    "#Esto lo que hace es procesar los datos de coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "coords = df[['origin_longitude', 'origin_latitude', 'destination_longitude', 'destination_latitude']].values #Combinamos coordenadas de origen y destino en una matriz.\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=10)# DBSCAN\n",
    "clusters = dbscan.fit_predict(coords)\n",
    "\n",
    "\n",
    "df['cluster'] = clusters#Se agregan las etiquetas de los clusters a los datos originales.\n",
    "\n",
    "\n",
    "'''\n",
    "Los viajes con orígenes y destinos similares serán agrupados en el mismo clúster. \n",
    "Esta solucion proporciona una agrupación más sofisticada y adaptable que la simple agrupación por códigos geohash y tiempo, especialmente si los patrones de viaje tienen una distribución densa y no uniforme.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Un servicio que es capaz de proporcionar la siguiente funcionalidad:\n",
    "a. Devuelve el promedio semanal de la cantidad de viajes para un área definida por un bounding box y la región.\n",
    "b. Informar sobre el estado de la ingesta de datos sin utilizar una solución de polling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lo primero es crear una una funcion que calcule el promedio. Asi mismo, podemos utilizar geopandas para detectar si un viaje esta dentro de un bounding box.\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "def obtener_promedio_semanal(df, bounding_box, region):\n",
    "    geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]#Crear un GeoDataFrame con el dataframe inicial.\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "    polygon = Polygon(bounding_box)#Se crea un polígono para el bounding box.\n",
    "    region_data = gdf[(gdf['region'] == region) & (gdf['geo'].within(polygon))]#Filtrar los datos por región y por ubicación\n",
    "\n",
    "    \n",
    "    region_data['week'] = region_data['datetime'].dt.week#Calcular el promedio semanal.\n",
    "    weekly_avg = region_data.groupby('week').size().mean()\n",
    "    return weekly_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el servicio es necesario tener un endpoint en la API que acepte un bounding box y una región como parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/promedio_semanal', methods=['GET'])\n",
    "def promedio_semanal():\n",
    "    bounding_box = request.args.get('bounding_box', type=list)#Obtener parámetros de la request\n",
    "\n",
    "    region = request.args.get('region', type=str)\n",
    "    \n",
    "    weekly_avg = obtener_promedio_semanal(df, bounding_box, region)#Calcular el promedio semanal\n",
    "\n",
    "    \n",
    "    return jsonify({'promedio_semanal': weekly_avg})#Devolver la respuesta como JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Para la ingesta de datos sin polling podemos usar websockets, que permiten una comunicación bidireccional entre el cliente y el servidor. \n",
    "A su vez, Flask-SocketIO facilita el uso de websockets en una aplicación Flask.\n",
    "'''\n",
    "\n",
    "from flask_socketio import SocketIO, emit\n",
    "\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app)\n",
    "@app.route('/ingesta_data', methods=['POST'])\n",
    "def ingest_data():\n",
    "    emit('ingesta_data', {'status': 'started'}, broadcast=True)#Ingesta de datos\n",
    "    try:\n",
    "        emit('ingesta_data', {'status': 'finished'}, broadcast=True)# Proceso de ingesta de datos...\n",
    "        emit('ingesta_data', {'status': 'error', 'message': str(e)}, broadcast=True)\n",
    "    return jsonify({'status': 'success'})\n",
    "\n",
    "'''\n",
    "Con esto los clientes/trabajadores podrían conectarse al websocket y recibir actualizaciones en tiempo real sobre el estado de la ingesta de datos.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer escalable la solución a 100 millones de entradas:\n",
    "\n",
    "Optimización de consultas a la base de datos: Las consultas a la base de datos deben estar optimizadas para asi extraer los datos de forma eficiente. \n",
    "Los índices en las columnas utilizadas con frecuencia para las consultas pueden ayudar a acelerar la recuperación de los datos. \n",
    "Además, asi se evita la selección de todas las columnas si solo necesitas unas pocas para tu análisis (Efectividad).\n",
    "\n",
    "Tecnologías adecuadas para manejar grandes cantidades de datos: Usando PostgreSQL, puedes beneficiarte de su capacidad para manejar grandes conjuntos de datos. \n",
    "A su vez, cuando trabajamos con cantidades enormes de datos, Spark y Hadoop pueden ser más adecuadas porque permiten el procesamiento distribuido de los datos\n",
    "\n",
    "Simplificación del modelo de datos: Para mejorar la eficiencia,se debe simplificar el modelo de datos lo maximo posible. \n",
    "Se puede eliminar de datos redundantes, la normalización de los datos para minimizar la duplicación, etc.\n",
    "\n",
    "Escalabilidad horizontal y vertical: La escalabilidad horizontal se refiere a la adición de más máquinas a tu pool de recursos, mientras que la escalabilidad vertical se refiere a la adición de más poder a una máquina existente. Ambas formas de escalabilidad pueden ayudar a manejar cargas de trabajo más grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Asi mismo, para probar si la solucion es optima en cuanto a escalabilidad .\n",
    "'''\n",
    "\n",
    "import time\n",
    "from sqlalchemy import insert\n",
    "\n",
    "data = [{'column1': 'value1', 'column2': 'value2', ...} for _ in range(100000000)]#Crear 100 millones de entradas\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "engine.execute(insert(my_table), data)\n",
    "print('Tiempo de inserción:', time.time() - start) #Medir el tiempo que lleva insertar 100 millones de entradas\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "engine.execute(my_table.select().where(my_table.c.column1 == 'value1')) #Medir el tiempo que lleva hacer una consulta\n",
    "print('Tiempo de consulta:', time.time() - start)\n",
    "\n",
    "'''Prueba de carga: Aquí, podriamos insertar 100 millones de entradas en la base de datos y medir cuánto tiempo lleva hacerlo. \n",
    "   ----\n",
    "   Prueba de escalabilidad: Podriamos aumentar gradualmente el volumen de datos en la base de datos y medir cómo cambia el rendimiento. \n",
    "   Por ejemplo, podríamos insertar 10 millones de entradas, medir el rendimiento, luego insertar otros 10 millones, medir el rendimiento, y así sucesivamente, hasta llegar a 100 millones de entradas.\n",
    "   ----\n",
    "   Prueba de estrés: Aquí, podriamos simular un alto nivel de demanda en la base de datos para ver cómo se comporta. \n",
    "   Esto podría implicar hacer muchas consultas a la base de datos en un corto período de tiempo.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el diagrama:\n",
    "\n",
    "1)Cloud Scheduler activa eventos en intervalos regulares.\n",
    "\n",
    "2)Estos eventos disparan mensajes en \"Cloud Pub/Sub\".\n",
    "\n",
    "3)Cuando un mensaje es publicado, activa tu \"App Python/Docker\" que está corriendo en \"Cloud Run\".\n",
    "\n",
    "4)La \"App Python/Docker\" procesa los datos y los almacena temporalmente en \"Cloud Storage\" si es necesario.\n",
    "\n",
    "5)Finalmente, los datos procesados se cargan en tu base de datos \"Cloud SQL\".\n",
    "\n",
    "IMPORTANTE MENCIONAR QUE EL DIAGRAMA SE INCLUIRA DE MANERA SIMPLE POR TEMAS DE TIEMPO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
